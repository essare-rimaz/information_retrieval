{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CREATING INVERTED INDEX FROM TOKENIZED TEXTS #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### conversion from strings to tokens within lists ###\n",
    "def Convert(string):\n",
    "    li = list(string.split(\" \"))\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### function to create an inverted index ###\n",
    "from collections import defaultdict\n",
    "def create_index (data):\n",
    "    index = defaultdict(list)\n",
    "    for i, tokens in enumerate(data):\n",
    "        for token in tokens:\n",
    "             index[token].append(i)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data processing ###\n",
    "import html\n",
    "import os, sys\n",
    "from lxml import etree\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# initialization\n",
    "kolekce = []\n",
    "hashed = []\n",
    "text_ready = []\n",
    "\n",
    "doc_ID_dict = {} # dict -> order of doc processed as key, DOCID as value\n",
    "\n",
    "# counters \n",
    "counter_child = 0\n",
    "counter_node = 0\n",
    "counter_doc_ID = 0\n",
    "counter_file = 0\n",
    "\n",
    "#path from where to load xml files\n",
    "path = \"/home/mazurri/InfoRetrieval\"\n",
    "directory = os.listdir(path)\n",
    "\n",
    "for file in directory:\n",
    "    counter_child = 0\n",
    "    counter_file = counter_file + 1\n",
    "    test = os.path.splitext(file)\n",
    "    if test[0] == \"queries_cs\":  # queries_cs is a xml file for the second part of the task -> queries\n",
    "        continue      \n",
    "    elif test[1] == \".xml\":\n",
    "        print(file)\n",
    "        #kvůli erroru s unescaped characters \"&\"\n",
    "        parser = etree.XMLParser(recover=True)\n",
    "        tree = ET.parse(file, parser=parser)\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            \n",
    "            ### populating doc_ID_dict\n",
    "            doc_ID = child.find(\"DOCID\").text\n",
    "            doc_ID_dict[counter_doc_ID] = doc_ID\n",
    "            counter_doc_ID = counter_doc_ID + 1\n",
    "            \n",
    "            for node in child:\n",
    "                if counter_node < 3: # first 3 nodes do not contain text to be processed\n",
    "                    counter_node = counter_node + 1\n",
    "                    continue\n",
    "                else:\n",
    "                    text = root[counter_child][counter_node].text\n",
    "                    \n",
    "                    counter_node = counter_node + 1\n",
    "\n",
    "                    ### normalization of texts ###\n",
    "                    text = text.lower()\n",
    "                    text = text.replace(\"\\n\", \" \")\n",
    "                    text = text.replace(\"?\", \" \")\n",
    "                    text = text.replace(\"!\", \" \")\n",
    "                    text = text.replace(\".\", \" \")\n",
    "                    text = text.replace(\",\", \" \")\n",
    "                    text = text.replace(\" - \", \" \")\n",
    "                    \n",
    "                    # tokenize as per desired format\n",
    "                    text = Convert(text)\n",
    "                    \n",
    "                    ### hashing the result tokens###\n",
    "                    for i in text:\n",
    "                        hashed.append(hash(i))\n",
    "                    \n",
    "            ### eliminating duplicates ###\n",
    "            hashed_set = set(hashed)\n",
    "            hashed = []\n",
    "\n",
    "            ### making a final set of tokens ###\n",
    "            kolekce.append(hashed_set)\n",
    "            hashed_set = None\n",
    "  \n",
    "            counter_node = 0\n",
    "            counter_child = counter_child + 1\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using inverted index to match tokens and counter_doc_ID ###\n",
    "inverted_index = create_index(kolekce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CREATING CUSTOM QUERY PARSER AND SEARCH ENGINE ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INPUT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operatory = [\"AND\", \"OR\", \"AND_NOT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWithinIndex (data):\n",
    "    data_new = data\n",
    "    token_index = 0\n",
    "    for i in data_new:\n",
    "        text = None\n",
    "        if i in operatory:\n",
    "            continue\n",
    "        else:\n",
    "            data_new[token_index] = inverted_index[hash(i.lower())]\n",
    "        token_index = token_index + 1\n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_findDocuments (data):\n",
    "    query_docs = []\n",
    "    query_docs_temp = []\n",
    "    for token_index, token in enumerate(data):\n",
    "        if token_index == 0:\n",
    "            for item in token:\n",
    "                query_docs.append(item)\n",
    "        elif token_index % 2 !=0:\n",
    "            \n",
    "            next_token = data[token_index+1]\n",
    "       \n",
    "            short_list = min(query_docs, next_token, key=len)\n",
    "            long_list = max(query_docs, next_token, key=len)\n",
    "            \n",
    "            if token == \"OR\":\n",
    "                query_docs = sorted(list(set(query_docs+next_token))) #připoj co následuje za OR do query_docs\n",
    "            \n",
    "            if token == \"AND\":\n",
    "                query_docs = sorted(list(set(short_list).intersection(set(long_list))))\n",
    "            \n",
    "            if token == \"AND_NOT\":\n",
    "                query_docs = sorted(list(set(short_list)-set(long_list)))               \n",
    "                \n",
    "        else:\n",
    "            continue\n",
    "    return query_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "counter_query_ID = 0\n",
    "\n",
    "operatory = [\"AND\", \"OR\", \"AND_NOT\"]\n",
    "\n",
    "for file in directory:\n",
    "    test = os.path.splitext(file)\n",
    "    if test[0] == \"queries_cs\":\n",
    "        parser = etree.XMLParser(recover=True)\n",
    "        tree = ET.parse(file, parser = parser)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        for counter_query_ID, child in enumerate(root):\n",
    "            doc_NUM = child.find(\"num\").text\n",
    "            query = child.find(\"query\").text            \n",
    "            query = re.sub(\"AND NOT\", \"AND_NOT\", query)\n",
    "\n",
    "            converted_query = Convert(query)\n",
    "\n",
    "            if \"\" in converted_query:\n",
    "                converted_query.remove(\"\")\n",
    "                \n",
    "            hashed_query = findWithinIndex(converted_query)\n",
    "            vysledek =  better_findDocuments(hashed_query)\n",
    "            \n",
    "            new_list = []\n",
    "            for item in vysledek:\n",
    "                if item > len(doc_ID_dict):\n",
    "                    new_list.append(item)\n",
    "                else:\n",
    "                    new_list.append(doc_ID_dict[item])\n",
    "            \n",
    "            full_str = ' '.join([str(elem) for elem in new_list])\n",
    "\n",
    "            # creating result file\n",
    "            with open(\"vysledky.txt\", \"a+\") as final_file:\n",
    "                final_file.seek(0)\n",
    "                data = final_file.read(100)\n",
    "                if len(data) > 0 :\n",
    "                    final_file.write(\"\\n\")\n",
    "                # Append text at the end of file\n",
    "                final_file.write(\"\\n\")\n",
    "                final_file.write(\"\\n\")\n",
    "                final_file.write(doc_NUM)\n",
    "                final_file.write(\"\\n\")\n",
    "                final_file.write(full_str)\n",
    "\n",
    "            new_list = []\n",
    "\n",
    "    else:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8",
   "language": "python",
   "name": "python-3.7.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
